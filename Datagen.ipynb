{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2a3f873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import gc\n",
    "import html\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import math\n",
    "import torchtext\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from torchsummary import summary\n",
    "from gensim.test.utils import common_texts\n",
    "import gensim.downloader\n",
    "from gensim.models import Word2Vec\n",
    "from transformers import AutoTokenizer, Data2VecTextModel\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78cb8be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_w2v_pretrained = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb22f2b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KeyedVectors' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgs_w2v_pretrained\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28mprint\u001b[39m(word)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyedVectors' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "\n",
    "for word in gs_w2v_pretrained.keys():\n",
    "    if word.startswith('<'):\n",
    "        print(word)\n",
    "vec = torch.as_tensor(gs_w2v_pretrained[word])\n",
    "print(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f37c7f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/home/csengehubay/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56aed29e1c444740976845d09b6331da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_dataset = load_dataset(\"wikitext\",\"wikitext-103-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a640b9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1801350/1801350 [00:42<00:00, 42843.96it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 3760/3760 [00:00<00:00, 42197.35it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 2461/2461 [00:00<00:00, 4802.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2445\n",
      "Val computed\n"
     ]
    }
   ],
   "source": [
    "train = []\n",
    "val = []\n",
    "\n",
    "train_sentences = []\n",
    "val_sentences = []\n",
    "\n",
    "\n",
    "for example in tqdm(raw_dataset['train']):\n",
    "    if len(example['text']) > 0:\n",
    "        train_sentences.append(example['text'])\n",
    "for example in tqdm(raw_dataset['validation']):\n",
    "    if len(example['text']) > 0:\n",
    "        val_sentences.append(example['text'])\n",
    "\n",
    "'''for sentence in tqdm(train_sentences[:388340]):\n",
    "    sentence_vec = []\n",
    "    for word in sentence.split(' '):\n",
    "        if '@' not in word and '=' not in word:\n",
    "            try:\n",
    "                sentence_vec.append(gs_w2v_pretrained [word])\n",
    "            except:\n",
    "                pass\n",
    "                #print(f'{word} not found')\n",
    "    #sentence_vec = [torch.from_numpy(item).float() for item in sentence_vec]\n",
    "    if len(sentence_vec) > 0:\n",
    "        sentence_vec = np.array(sentence_vec)\n",
    "        train.append(torch.as_tensor(sentence_vec))\n",
    "\n",
    "torch.save(train,'data/train_1.pt')\n",
    "del train \n",
    "gc.collect()'''\n",
    "\n",
    "'''train = []\n",
    "for sentence in tqdm(train_sentences[388340:776690]):\n",
    "    sentence_vec = []\n",
    "    for word in sentence.split(' '):\n",
    "        if '@' not in word and '=' not in word:\n",
    "            try:\n",
    "                sentence_vec.append(gs_w2v_pretrained [word])\n",
    "            except:\n",
    "                pass\n",
    "                #print(f'{word} not found')\n",
    "    #sentence_vec = [torch.from_numpy(item).float() for item in sentence_vec]\n",
    "    if len(sentence_vec) > 0:\n",
    "        sentence_vec = np.array(sentence_vec)\n",
    "        train.append(torch.as_tensor(sentence_vec))\n",
    "\n",
    "torch.save(train,'data/train_2.pt')\n",
    "del train \n",
    "gc.collect()'''\n",
    "\n",
    "'''train = []\n",
    "for sentence in tqdm(train_sentences[776690:]):\n",
    "    sentence_vec = []\n",
    "    for word in sentence.split(' '):\n",
    "        if '@' not in word and '=' not in word:\n",
    "            try:\n",
    "                sentence_vec.append(gs_w2v_pretrained [word])\n",
    "            except:\n",
    "                pass\n",
    "                #print(f'{word} not found')\n",
    "    #sentence_vec = [torch.from_numpy(item).float() for item in sentence_vec]\n",
    "    if len(sentence_vec) > 0:\n",
    "        sentence_vec = np.array(sentence_vec)\n",
    "        train.append(torch.as_tensor(sentence_vec))\n",
    "\n",
    "torch.save(train,'data/train_3.pt')\n",
    "del train \n",
    "gc.collect()\n",
    "\n",
    "print('Train computed')'''\n",
    "\n",
    "for sentence in tqdm(val_sentences):\n",
    "    sentence_vec = []\n",
    "    for word in sentence.split(' '):\n",
    "        if '@' not in word and '=' not in word:\n",
    "            try:\n",
    "                sentence_vec.append(gs_w2v_pretrained [word])\n",
    "            except:\n",
    "                pass\n",
    "                #print(f'{word} not found')\n",
    "    #sentence_vec = [torch.from_numpy(item).float() for item in sentence_vec]\n",
    "    if len(sentence_vec) > 0:\n",
    "        sentence_vec = np.array(sentence_vec)\n",
    "        val.append(torch.as_tensor(sentence_vec))\n",
    "print(len(val))\n",
    "torch.save(val,'data/val.pt')\n",
    "print('Val computed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "613c3cc4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m in_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/train.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m data_1 \u001b[38;5;241m=\u001b[39m data[:\u001b[38;5;241m388300\u001b[39m,:,:]\n\u001b[1;32m      5\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(val,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/train_1.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:809\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    808\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 809\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[1;32m    811\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:1172\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1170\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1171\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1172\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1174\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:1142\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1141\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1142\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:1112\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_tensor\u001b[39m(dtype, numel, key, location):\n\u001b[1;32m   1110\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m-> 1112\u001b[0m     storage \u001b[38;5;241m=\u001b[39m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_storage_from_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_untyped_storage\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m     \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[1;32m   1116\u001b[0m         wrap_storage\u001b[38;5;241m=\u001b[39mrestore_location(storage, location),\n\u001b[1;32m   1117\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1118\u001b[0m         _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "in_path = 'data/train.pt'\n",
    "data = torch.load(in_path)\n",
    "\n",
    "data_1 = data[:388300,:,:]\n",
    "torch.save(val,'data/train_1.pt')\n",
    "del data_1\n",
    "gc.collect()\n",
    "print('First third saved')\n",
    "\n",
    "data_1 = data[388300:776690,:,:]\n",
    "torch.save(val,'data/train_2.pt')\n",
    "del data_1\n",
    "gc.collect()\n",
    "print('Second third saved')\n",
    "\n",
    "data_1 = data[776690:,:,:]\n",
    "torch.save(val,'data/train_3.pt')\n",
    "del data_1\n",
    "gc.collect()\n",
    "print('All saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c967416",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
